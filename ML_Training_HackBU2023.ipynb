{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6336b67",
      "metadata": {
        "id": "e6336b67",
        "outputId": "ba813041-de8b-4d12-9fb5-bc1475f63735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Size: 0.01\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Bad       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         1\n",
            "   macro avg       1.00      1.00      1.00         1\n",
            "weighted avg       1.00      1.00      1.00         1\n",
            "\n",
            "Test Size: 0.11\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Bad       0.50      0.80      0.62         5\n",
            "        Good       0.67      0.33      0.44         6\n",
            "\n",
            "    accuracy                           0.55        11\n",
            "   macro avg       0.58      0.57      0.53        11\n",
            "weighted avg       0.59      0.55      0.52        11\n",
            "\n",
            "Test Size: 0.21\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Bad       0.64      1.00      0.78         9\n",
            "        Good       1.00      0.58      0.74        12\n",
            "\n",
            "    accuracy                           0.76        21\n",
            "   macro avg       0.82      0.79      0.76        21\n",
            "weighted avg       0.85      0.76      0.76        21\n",
            "\n",
            "Test Size: 0.31\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Bad       0.61      0.85      0.71        13\n",
            "        Good       0.85      0.61      0.71        18\n",
            "\n",
            "    accuracy                           0.71        31\n",
            "   macro avg       0.73      0.73      0.71        31\n",
            "weighted avg       0.75      0.71      0.71        31\n",
            "\n",
            "Test Size: 0.41\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Bad       0.61      0.78      0.68        18\n",
            "        Good       0.78      0.61      0.68        23\n",
            "\n",
            "    accuracy                           0.68        41\n",
            "   macro avg       0.69      0.69      0.68        41\n",
            "weighted avg       0.70      0.68      0.68        41\n",
            "\n",
            "Test Size: 0.51\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Bad       0.53      0.76      0.63        21\n",
            "        Good       0.76      0.53      0.63        30\n",
            "\n",
            "    accuracy                           0.63        51\n",
            "   macro avg       0.65      0.65      0.63        51\n",
            "weighted avg       0.67      0.63      0.63        51\n",
            "\n",
            "Test Size: 0.61\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Bad       0.54      0.83      0.66        24\n",
            "        Good       0.83      0.54      0.66        37\n",
            "\n",
            "    accuracy                           0.66        61\n",
            "   macro avg       0.69      0.69      0.66        61\n",
            "weighted avg       0.72      0.66      0.66        61\n",
            "\n",
            "Test Size: 0.71\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Bad       0.60      0.83      0.69        30\n",
            "        Good       0.83      0.59      0.69        41\n",
            "\n",
            "    accuracy                           0.69        71\n",
            "   macro avg       0.71      0.71      0.69        71\n",
            "weighted avg       0.73      0.69      0.69        71\n",
            "\n",
            "Test Size: 0.81\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Bad       0.58      0.78      0.67        36\n",
            "        Good       0.76      0.56      0.64        45\n",
            "\n",
            "    accuracy                           0.65        81\n",
            "   macro avg       0.67      0.67      0.65        81\n",
            "weighted avg       0.68      0.65      0.65        81\n",
            "\n",
            "Test Size: 0.91\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Bad       0.62      0.76      0.68        42\n",
            "        Good       0.74      0.59      0.66        49\n",
            "\n",
            "    accuracy                           0.67        91\n",
            "   macro avg       0.68      0.68      0.67        91\n",
            "weighted avg       0.68      0.67      0.67        91\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "df = pd.read_csv(\"final.csv\")\n",
        "\n",
        "X = df.drop(columns = [\"url\", \"https\", \"status\",\"bad_words\", \"tld\", \"label\"])\n",
        "y = df[\"label\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42) #Here we are allocating 20% of our data for testing\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test) # scales the attributes to weigh equally\n",
        "\n",
        "rfc = RandomForestClassifier(n_estimators = 100)\n",
        "rfc.fit(X_train, y_train)\n",
        "pred_rfc = rfc.predict(X_test)\n",
        "pred_rfc[:10]\n",
        "\n",
        "i = 1\n",
        "while i < 100:\n",
        "    size = i/100;\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = i, random_state = 42)\n",
        "    X_train = sc.fit_transform(X_train)\n",
        "    X_test = sc.transform(X_test) # scales the attributes to weigh equally\n",
        "\n",
        "    rfc = RandomForestClassifier(n_estimators = 100)\n",
        "    rfc.fit(X_train, y_train)\n",
        "    pred_rfc = rfc.predict(X_test)\n",
        "    print(\"Test Size: {0}\".format(size))\n",
        "    print(classification_report(y_test, pred_rfc))\n",
        "    i+=10\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}